{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Twitter Sentiment Analysis\n",
    "\n",
    "In this notebook, to predict the sentiment of a tweet, we use the algorithm : \n",
    "\n",
    "- Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurent/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import ConfusionMatrix\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataSet : TRAIN & TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/laurent/Documents/Data/TextClassificationGit/train_E6oV3lV.csv\", encoding='ISO-8859-1')\n",
    "test = pd.read_csv(\"/Users/laurent/Documents/Data/TextClassificationGit/test_tweets_anuFYb8.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.tweet = train.tweet.str.replace(r\"can't\", \"cannot \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\'ve\", \" have \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\'s\", \" \")\n",
    "train.tweet = train.tweet.str.replace(r\"n't\", \" not \")\n",
    "train.tweet = train.tweet.str.replace(r\"i'm\", \"I am\")\n",
    "train.tweet = train.tweet.str.replace(r\"\\'re\", \" are \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\'d\", \" would \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\rly\", \" really \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\ gp\", \" grand prix \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\ yeeesss\", \" yes \")\n",
    "train.tweet = train.tweet.str.replace(r\"\\ pt\", \" point \")\n",
    "\n",
    "test.tweet = test.tweet.str.replace(r\"can't\", \"cannot \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\'ve\", \" have \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\'s\", \" \")\n",
    "test.tweet = test.tweet.str.replace(r\"n't\", \" not \")\n",
    "test.tweet = test.tweet.str.replace(r\"i'm\", \"I am\")\n",
    "test.tweet = test.tweet.str.replace(r\"\\'re\", \" are \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\'d\", \" would \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\rly\", \" really \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\ gp\", \" grand prix \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\ yeeesss\", \" yes \")\n",
    "test.tweet = test.tweet.str.replace(r\"\\ pt\", \" point \")\n",
    "\n",
    "train_label = train['label'] == 1\n",
    "df_try = train[train_label]\n",
    "train = train.append([df_try]*10, ignore_index = True)\n",
    "\n",
    "train.sample(frac=1)\n",
    "\n",
    "training = train.tweet\n",
    "dftext = []\n",
    "for t in training:\n",
    "    dftext.append(tweet_cleaner(t))\n",
    "    \n",
    "testing = test.tweet\n",
    "dftest = []\n",
    "for t in testing:\n",
    "    dftest.append(tweet_cleaner(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfrom Train label to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfList = train['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Train tweet and label in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = sklearn.datasets.base.Bunch(data=dftext, target=dfList)\n",
    "train = pd.DataFrame(dataset)\n",
    "train.columns = ['tweet','label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',   CountVectorizer(stop_words = 'english', lowercase=True,binary= False,max_df= 0.5,max_features= 50000, ngram_range = (1, 2))),\n",
    "    ('tfidf_transformer',  TfidfTransformer(norm = 'l2', use_idf = True, sublinear_tf=True)),\n",
    "    ('classifier',         LinearSVC(tol = 0.1, max_iter = 100))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model with Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets classified: 54382\n",
      "Score: 0.95854969325\n",
      "Confusion matrix:\n",
      "[[29357   363]\n",
      " [   65 24597]]\n",
      "CPU times: user 11.2 s, sys: 300 ms, total: 11.5 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k_fold = KFold(n=len(train), n_folds=6)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = train.iloc[train_indices]['tweet'].values\n",
    "    train_y = train.iloc[train_indices]['label'].values\n",
    "\n",
    "    test_text = train.iloc[test_indices]['tweet'].values\n",
    "    test_y = train.iloc[test_indices]['label'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, average= 'binary')\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total tweets classified:', len(train))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(dftest)\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [test, predictions]\n",
    "submission = pd.concat(frames, axis=1, join_axes=[test.index])\n",
    "submission\n",
    "submission = submission[['id',0]]\n",
    "submission.describe()\n",
    "submission.columns = ['id', 'label']\n",
    "submission = submission[['id', 'label']]\n",
    "submission.to_csv('sub21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',   CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('clf',  LinearSVC())\n",
    "])\n",
    "parameters = {  \n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),  \n",
    "    'clf__tol' : (0.1, 0.01, 0.001, 0.0001),\n",
    "    'clf__max_iter' : (100, 500, 1000, 2000)\n",
    "    } \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000, 10000, 50000), 'vect__ngram_range': ((1, 1), (1, 2)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__tol': (0.1, 0.01, 0.001, 0.0001), 'clf__max_iter': (100, 500, 1000, 2000)}\n",
      "Fitting 3 folds for each of 1536 candidates, totalling 4608 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 31.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 40.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4608 out of 4608 | elapsed: 46.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.994\n",
      "Best parameters set:\n",
      "\tclf__max_iter: 100\n",
      "\tclf__tol: 0.1\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: 50000\n",
      "\tvect__ngram_range: (1, 2)\n",
      "CPU times: user 1min 46s, sys: 23.4 s, total: 2min 9s\n",
      "Wall time: 46min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters)\n",
    "    #t0 = time()\n",
    "    grid_search.fit(train.tweet, train.label)\n",
    "    #print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
